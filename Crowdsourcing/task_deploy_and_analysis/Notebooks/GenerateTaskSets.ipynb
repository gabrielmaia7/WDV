{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import traceback\n",
    "import math\n",
    "import ast\n",
    "import pandas as pd\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_microtask_dataframe_to_json_set(df):\n",
    "    microtask_jsons = []\n",
    "    try:\n",
    "        for idx, row in df.iterrows():\n",
    "            print(round((idx+1)/df.shape[0]*100,2),'%',' '*15, end='\\r')\n",
    "            microtask_json = {\n",
    "                \"claim_id\": row['claim_id'],\n",
    "                \"verbalised_claim\": row['processed_verbalisation'],\n",
    "                \"subject_label\": row['entity_label'],\n",
    "                \"subject_description\": row['entity_desc'] if row['entity_desc'] != 'no-desc' else \"No description available\",\n",
    "                \"subject_aliases\": ast.literal_eval(row['entity_alias']) if row['entity_alias'] != 'no-alias' else \"No aliases available\",\n",
    "                \"predicate_label\": row['property_label'],\n",
    "                \"predicate_description\": row['property_desc'] if row['property_desc'] != 'no-desc' else \"No description available\",\n",
    "                \"predicate_aliases\": ast.literal_eval(row['property_alias']) if row['property_alias'] != 'no-alias' else \"No aliases available\",\n",
    "                \"object_label\": row['object_label'],\n",
    "                \"object_description\": row['object_desc'] if row['object_desc'] != 'no-desc' else \"No description available\",\n",
    "                \"object_aliases\": ast.literal_eval(row['object_alias']) if row['object_alias'] != 'no-alias' else \"No aliases available\",\n",
    "                \"g_id\": 0\n",
    "            }\n",
    "            microtask_jsons.append(microtask_json)\n",
    "        return microtask_jsons\n",
    "    except Exception:\n",
    "        print(row)\n",
    "        traceback.print_exc()\n",
    "        pdb.set_trace()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxSingleInstances = 1 #MAX TIMES ANY REFERENCE APPEARS AMONG THE TASK SETS\n",
    "#maxSingleInstances_gd = 2\n",
    "\n",
    "def getRandomTask(counter, n, non_gd_references, maxSingleInstances):\n",
    "    p = [max(maxSingleInstances-c,0.0001) for c in counter]\n",
    "    p = [pp/sum(p) for pp in p]\n",
    "    indexes = np.random.choice(\n",
    "        a = list(range(len(non_gd_references))),\n",
    "        size=n,\n",
    "        replace=False,\n",
    "        p=p\n",
    "    )        \n",
    "    task = []\n",
    "    for i in indexes:\n",
    "        reference = non_gd_references[i]\n",
    "        task.append((reference,i))\n",
    "    return task\n",
    "\n",
    "def getRandomGoldenTask(counter, n, gd_references, maxSingleInstances_gd):    \n",
    "    p = [max(maxSingleInstances_gd-c,0.0001) for c in counter]\n",
    "    p = [pp/sum(p) for pp in p]\n",
    "    indexes = np.random.choice(\n",
    "        a = list(range(len(gd_references))),\n",
    "        size=n,\n",
    "        replace=False,\n",
    "        p=p\n",
    "    )\n",
    "    task = []\n",
    "    for i in indexes:\n",
    "        reference = gd_references[i]\n",
    "        task.append((reference,i))\n",
    "    return task\n",
    "\n",
    "def generateTaskSet(counter, counter_gd, non_gd_references, gd_references, maxSingleInstances, maxSingleInstances_gd, n=(4,2)):\n",
    "    '''\n",
    "    counter = a counter which keeps track of how many times each reference was retrieved\n",
    "    n = (x,y) where x = number of non_gd references and y = number of gd references\n",
    "    '''\n",
    "    taskSet = []\n",
    "    task = getRandomTask(counter, n[0], non_gd_references, maxSingleInstances)\n",
    "    taskSet += [r for (r,i) in task] # pairs reference,index are generated here, so that we can update the counter later\n",
    "    task_gd = getRandomGoldenTask(counter_gd, n[1], gd_references, maxSingleInstances_gd)\n",
    "    taskSet += [r for (r,i) in task_gd]\n",
    "    random.shuffle(taskSet)\n",
    "    return taskSet, [i for (p,i) in task], [i for (p,i) in task_gd] # we return indices here to update counter\n",
    "\n",
    "\n",
    "def generateIDdTaskSets(non_gd_references, gd_references, maxSingleInstances, maxSingleInstances_gd):\n",
    "    counter = [0]*len(non_gd_references)\n",
    "    counter_gd = [0]*len(gd_references)\n",
    "    taskSets = []\n",
    "    while (any([c < maxSingleInstances for c in counter])):\n",
    "        taskSet, indexes, indexes_gd = generateTaskSet(counter, counter_gd, non_gd_references,\n",
    "                                                       gd_references, maxSingleInstances, maxSingleInstances_gd)\n",
    "        taskSetIDd = {\n",
    "            '_id': str(uuid.uuid4()),\n",
    "            'taskSet' : taskSet\n",
    "        }\n",
    "        taskSets.append(taskSetIDd)\n",
    "        for i in indexes:\n",
    "            counter[i] = counter[i] + 1\n",
    "        for i in indexes_gd:\n",
    "            counter_gd[i] = counter_gd[i] + 1\n",
    "    return taskSets, counter, counter_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PILOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('./data/pilot/pilot_sampled_df_verbalised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   entity_id                64 non-null     object \n",
      " 1   claim_id                 64 non-null     object \n",
      " 2   rank                     64 non-null     object \n",
      " 3   property_id              64 non-null     object \n",
      " 4   datatype                 64 non-null     object \n",
      " 5   datavalue                64 non-null     object \n",
      " 6   sampling_weight_vb       64 non-null     object \n",
      " 7   sampling_weight          64 non-null     float64\n",
      " 8   entity_label             64 non-null     object \n",
      " 9   property_label           64 non-null     object \n",
      " 10  object_label             64 non-null     object \n",
      " 11  theme_entity_id          64 non-null     object \n",
      " 12  theme_entity_label       64 non-null     object \n",
      " 13  entity_desc              64 non-null     object \n",
      " 14  property_desc            64 non-null     object \n",
      " 15  object_desc              64 non-null     object \n",
      " 16  entity_alias             64 non-null     object \n",
      " 17  property_alias           64 non-null     object \n",
      " 18  object_alias             64 non-null     object \n",
      " 19  entity_label_lan         64 non-null     object \n",
      " 20  property_label_lan       64 non-null     object \n",
      " 21  object_label_lan         64 non-null     object \n",
      " 22  entity_desc_lan          64 non-null     object \n",
      " 23  property_desc_lan        64 non-null     object \n",
      " 24  object_desc_lan          64 non-null     object \n",
      " 25  entity_alias_lan         64 non-null     object \n",
      " 26  property_alias_lan       64 non-null     object \n",
      " 27  object_alias_lan         64 non-null     object \n",
      " 28  verbalisation            64 non-null     object \n",
      " 29  processed_verbalisation  64 non-null     object \n",
      "dtypes: float64(1), object(29)\n",
      "memory usage: 15.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to\n",
    "- [ ] CHECK FOR TRIPLES WITH NO LABEL AT ALL!!\n",
    "- [ ] MAKE BAD GOLDEN CASES EVEN MORE NOTICIABLY BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.56 %                \r",
      "3.12 %                \r",
      "4.69 %                \r",
      "6.25 %                \r",
      "7.81 %                \r",
      "9.38 %                \r",
      "10.94 %                \r",
      "12.5 %                \r",
      "14.06 %                \r",
      "15.62 %                \r",
      "17.19 %                \r",
      "18.75 %                \r",
      "20.31 %                \r",
      "21.88 %                \r",
      "23.44 %                \r",
      "25.0 %                \r",
      "26.56 %                \r",
      "28.12 %                \r",
      "29.69 %                \r",
      "31.25 %                \r",
      "32.81 %                \r",
      "34.38 %                \r",
      "35.94 %                \r",
      "37.5 %                \r",
      "39.06 %                \r",
      "40.62 %                \r",
      "42.19 %                \r",
      "43.75 %                \r",
      "45.31 %                \r",
      "46.88 %                \r",
      "48.44 %                \r",
      "50.0 %                \r",
      "51.56 %                \r",
      "53.12 %                \r",
      "54.69 %                \r",
      "56.25 %                \r",
      "57.81 %                \r",
      "59.38 %                \r",
      "60.94 %                \r",
      "62.5 %                \r",
      "64.06 %                \r",
      "65.62 %                \r",
      "67.19 %                \r",
      "68.75 %                \r",
      "70.31 %                \r",
      "71.88 %                \r",
      "73.44 %                \r",
      "75.0 %                \r",
      "76.56 %                \r",
      "78.12 %                \r",
      "79.69 %                \r",
      "81.25 %                \r",
      "82.81 %                \r",
      "84.38 %                \r",
      "85.94 %                \r",
      "87.5 %                \r",
      "89.06 %                \r",
      "90.62 %                \r",
      "92.19 %                \r",
      "93.75 %                \r",
      "95.31 %                \r",
      "96.88 %                \r",
      "98.44 %                \r",
      "100.0 %                \r"
     ]
    }
   ],
   "source": [
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Gold Standard\n",
    "\n",
    "We now split the json sets into two, one with golden standards and one with references to be judged.\n",
    "\n",
    "Here is we calculate an appropriate number of gold standards:\n",
    "- Let X be the maximum expected number of tasks any singular worker can complete;\n",
    "- Let Y be the number of gold standard references we have annotated;\n",
    "- Let Z be the number of combinations of 2 gold standard references that we can take from the Y gold standard references in our gold set, regardless of pairing order, without repeating.\n",
    "- Let P be the probability of a worker doing X tasks and not finding a repeated pair of golden standard references, taken from the set of Z gold standard pairs.\n",
    "\n",
    "So we calculate:\n",
    "- Z = Y*(Y-1)/2\n",
    "- P = Z!/((Z^X)*(Z-X)!)\n",
    "\n",
    "So, we set X = 15 (The mean number of tasks per worker for previous experiments has been 4, with standard deviation of 10.5, so if it follows a normal distribution, we get 85% of workers here).\n",
    "\n",
    "According to the calculation below, we see that **Y = 45** gives us near 90% of chances of not having repeated gold standard, so that is how many gold standards we aim at annotating for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2859b08fdf0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "from decimal import *\n",
    "import matplotlib.pyplot as plt\n",
    "def fac(x):\n",
    "    return Decimal(math.factorial(x))\n",
    "x = 15\n",
    "ys = list(range(15,100))\n",
    "zs = [y*(y-1)/2 for y in ys]\n",
    "ps = [fac(z)/(Decimal(z**x)*fac(z-x)) for z in zs]\n",
    "plt.plot(ys,ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-6e66c7b2be24>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-6e66c7b2be24>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    with open('data/pilot/verbalisations.json','w+',encoding='utf8') as f:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "golden_standard_quota = 6 #should be 45 but we are using 6 for the pilot\n",
    "\n",
    "random.shuffle(jsonsets)\n",
    "#with open('data/pilot/verbalisations_gd.json','w+',encoding='utf8') as f:\n",
    "    #json.dump(jsonsets[:golden_standard_quota], f, indent=2,ensure_ascii=False)\n",
    "#with open('data/pilot/verbalisations.json','w+',encoding='utf8') as f:\n",
    "    #json.dump(jsonsets[golden_standard_quota:], f, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must now annotate the golden standard by doing the following:\n",
    "For each gd reference in the gd json set, go to the \"gd\" field, which should be like this:\n",
    "```\n",
    "{\n",
    "    \"reference_id\": [random uuid],\n",
    "    \"url\": [an url],\n",
    "    ...,\n",
    "    \"g_id\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "and change it to something of the format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"reference_id\": [random uuid],\n",
    "    \"url\": [an url],\n",
    "    ...,\n",
    "    \"g_id\": {\n",
    "        \"relevance\": {\n",
    "          \"is_present\": [0 if it is not present, 1 if it is],\n",
    "          \"difficulty\":[a list of numbers from 0 to 4, with 0 to 4 simbolizing possible difficulty answers as seen in the task mockup. Can also be just -1 if it does not matter.],\n",
    "          \"reason\":[a list of numbers from 0 to 6, with 0 to 6 simbolizing possible reason answers as seen in the task mockup. Can also be just -1 if it does not matter.]\n",
    "        },\n",
    "        \"authorit\":{\n",
    "          \"author\": [a number from 0 to 2 simbolizing the author type as seen in the task mockup],\n",
    "          \"publisher\": [a number from 0 to 4 simbolizing the publisher type as seen in the task mockup],\n",
    "          \"sub_publisher\": [a number from 0 to the amount of subpublisher types for this publisher type, simbolizing the subpublisher type as seen in the task mockup. It can also be -1 if you don't care for this field.]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Task Sets\n",
    "\n",
    "**Make sure you have annotated the GD before proceeding here**\n",
    "\n",
    "**Make sure you have also filtered the non-GD for API-verifiable examples**\n",
    "\n",
    "Now we take 4 non_gd references and 2 gd references and pack them into task sets of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets\n",
      "Gd len: 16\n",
      "Non-Gd len: 58\n",
      "Generated 15 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1]\n",
      "Each golden data appearance counter is: [2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Generating Task Sets')\n",
    "with open('data/pilot/verbalisations_gd_2.json','r',encoding='utf8') as f:\n",
    "    verbalisations_gd = json.load(f)\n",
    "    print('Gd len:',len(verbalisations_gd))\n",
    "with open('data/pilot/verbalisations.json','r',encoding='utf8') as f:\n",
    "    verbalisations = json.load(f)\n",
    "    print('Non-Gd len:',len(verbalisations))\n",
    "\n",
    "ds, c, c_gd = generateIDdTaskSets(verbalisations, verbalisations_gd)\n",
    "print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "\n",
    "#with open('./data/pilot/TaskSets.json','w+',encoding='utf8') as f:\n",
    "#    json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAMPAIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6651 entries, 0 to 6650\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   entity_id                6651 non-null   object \n",
      " 1   claim_id                 6651 non-null   object \n",
      " 2   rank                     6651 non-null   object \n",
      " 3   property_id              6651 non-null   object \n",
      " 4   datatype                 6651 non-null   object \n",
      " 5   datavalue                6651 non-null   object \n",
      " 6   sampling_weight_vb       6651 non-null   object \n",
      " 7   sampling_weight          6651 non-null   float64\n",
      " 8   entity_label             6651 non-null   object \n",
      " 9   property_label           6651 non-null   object \n",
      " 10  object_label             6651 non-null   object \n",
      " 11  theme_entity_id          6651 non-null   object \n",
      " 12  theme_entity_label       6651 non-null   object \n",
      " 13  entity_desc              6651 non-null   object \n",
      " 14  property_desc            6651 non-null   object \n",
      " 15  object_desc              6651 non-null   object \n",
      " 16  entity_alias             6651 non-null   object \n",
      " 17  property_alias           6651 non-null   object \n",
      " 18  object_alias             6651 non-null   object \n",
      " 19  entity_label_lan         6651 non-null   object \n",
      " 20  property_label_lan       6651 non-null   object \n",
      " 21  object_label_lan         6651 non-null   object \n",
      " 22  entity_desc_lan          6651 non-null   object \n",
      " 23  property_desc_lan        6651 non-null   object \n",
      " 24  object_desc_lan          6651 non-null   object \n",
      " 25  entity_alias_lan         6651 non-null   object \n",
      " 26  property_alias_lan       6651 non-null   object \n",
      " 27  object_alias_lan         6651 non-null   object \n",
      " 28  verbalisation            6651 non-null   object \n",
      " 29  processed_verbalisation  6651 non-null   object \n",
      " 30  unk_count                6651 non-null   int64  \n",
      " 31  property_and_theme_id    6651 non-null   object \n",
      " 32  campaign_group           6651 non-null   int64  \n",
      "dtypes: float64(1), int64(2), object(30)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('./data/campaign/campaign_sampled_df_verbalised_english.csv')\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     159\n",
       "1     159\n",
       "4     159\n",
       "8     159\n",
       "12    159\n",
       "13    159\n",
       "9     159\n",
       "11    159\n",
       "7     159\n",
       "3     159\n",
       "5     159\n",
       "2     159\n",
       "6     159\n",
       "10    159\n",
       "14    159\n",
       "32    158\n",
       "40    158\n",
       "36    158\n",
       "21    158\n",
       "28    158\n",
       "24    158\n",
       "20    158\n",
       "16    158\n",
       "17    158\n",
       "39    158\n",
       "25    158\n",
       "34    158\n",
       "31    158\n",
       "27    158\n",
       "23    158\n",
       "19    158\n",
       "15    158\n",
       "38    158\n",
       "30    158\n",
       "29    158\n",
       "26    158\n",
       "22    158\n",
       "18    158\n",
       "41    158\n",
       "35    158\n",
       "33    158\n",
       "37    158\n",
       "Name: campaign_group, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.campaign_group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Artificial Golden Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_remaining = data_df[data_df.campaign_group >= 3].reset_index(drop=True) # Taken from not campaigns groups 0,1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 45 golden data points with poor fluency\n",
    "\n",
    "Select these apart and then manually alter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.22 %                \r",
      "4.44 %                \r",
      "6.67 %                \r",
      "8.89 %                \r",
      "11.11 %                \r",
      "13.33 %                \r",
      "15.56 %                \r",
      "17.78 %                \r",
      "20.0 %                \r",
      "22.22 %                \r",
      "24.44 %                \r",
      "26.67 %                \r",
      "28.89 %                \r",
      "31.11 %                \r",
      "33.33 %                \r",
      "35.56 %                \r",
      "37.78 %                \r",
      "40.0 %                \r",
      "42.22 %                \r",
      "44.44 %                \r",
      "46.67 %                \r",
      "48.89 %                \r",
      "51.11 %                \r",
      "53.33 %                \r",
      "55.56 %                \r",
      "57.78 %                \r",
      "60.0 %                \r",
      "62.22 %                \r",
      "64.44 %                \r",
      "66.67 %                \r",
      "68.89 %                \r",
      "71.11 %                \r",
      "73.33 %                \r",
      "75.56 %                \r",
      "77.78 %                \r",
      "80.0 %                \r",
      "82.22 %                \r",
      "84.44 %                \r",
      "86.67 %                \r",
      "88.89 %                \r",
      "91.11 %                \r",
      "93.33 %                \r",
      "95.56 %                \r",
      "97.78 %                \r",
      "100.0 %                \r"
     ]
    }
   ],
   "source": [
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_remaining.sample(45, random_state=42).reset_index(drop=True))\n",
    "\n",
    "for jsonset in jsonsets:\n",
    "    jsonset['g_id'] = {\n",
    "      \"fluency\": [0,1,2],\n",
    "      \"adequacy\": [0,1,2]\n",
    "    }  \n",
    "    jsonset['claim_id'] = jsonset['claim_id'] '_GD_PLUS_FLUENCY'\n",
    "    jsonset['verbalised_claim'] = jsonset['verbalised_claim'] + ' $ALTER THIS'\n",
    "    \n",
    "with open('data/campaign/batch_1/verbalisations_gd_plus_fluency.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets[:golden_standard_quota], f, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 45 golden data points with poor adequacy\n",
    "\n",
    "Triples are paired with random verbalisations from other triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.22 %                \r",
      "4.44 %                \r",
      "6.67 %                \r",
      "8.89 %                \r",
      "11.11 %                \r",
      "13.33 %                \r",
      "15.56 %                \r",
      "17.78 %                \r",
      "20.0 %                \r",
      "22.22 %                \r",
      "24.44 %                \r",
      "26.67 %                \r",
      "28.89 %                \r",
      "31.11 %                \r",
      "33.33 %                \r",
      "35.56 %                \r",
      "37.78 %                \r",
      "40.0 %                \r",
      "42.22 %                \r",
      "44.44 %                \r",
      "46.67 %                \r",
      "48.89 %                \r",
      "51.11 %                \r",
      "53.33 %                \r",
      "55.56 %                \r",
      "57.78 %                \r",
      "60.0 %                \r",
      "62.22 %                \r",
      "64.44 %                \r",
      "66.67 %                \r",
      "68.89 %                \r",
      "71.11 %                \r",
      "73.33 %                \r",
      "75.56 %                \r",
      "77.78 %                \r",
      "80.0 %                \r",
      "82.22 %                \r",
      "84.44 %                \r",
      "86.67 %                \r",
      "88.89 %                \r",
      "91.11 %                \r",
      "93.33 %                \r",
      "95.56 %                \r",
      "97.78 %                \r",
      "100.0 %                \r",
      "2.22 %                \r",
      "4.44 %                \r",
      "6.67 %                \r",
      "8.89 %                \r",
      "11.11 %                \r",
      "13.33 %                \r",
      "15.56 %                \r",
      "17.78 %                \r",
      "20.0 %                \r",
      "22.22 %                \r",
      "24.44 %                \r",
      "26.67 %                \r",
      "28.89 %                \r",
      "31.11 %                \r",
      "33.33 %                \r",
      "35.56 %                \r",
      "37.78 %                \r",
      "40.0 %                \r",
      "42.22 %                \r",
      "44.44 %                \r",
      "46.67 %                \r",
      "48.89 %                \r",
      "51.11 %                \r",
      "53.33 %                \r",
      "55.56 %                \r",
      "57.78 %                \r",
      "60.0 %                \r",
      "62.22 %                \r",
      "64.44 %                \r",
      "66.67 %                \r",
      "68.89 %                \r",
      "71.11 %                \r",
      "73.33 %                \r",
      "75.56 %                \r",
      "77.78 %                \r",
      "80.0 %                \r",
      "82.22 %                \r",
      "84.44 %                \r",
      "86.67 %                \r",
      "88.89 %                \r",
      "91.11 %                \r",
      "93.33 %                \r",
      "95.56 %                \r",
      "97.78 %                \r",
      "100.0 %                \r"
     ]
    }
   ],
   "source": [
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_remaining.sample(45, random_state=24783).reset_index(drop=True))\n",
    "jsonsets_2 = convert_microtask_dataframe_to_json_set(data_df_remaining.sample(45, random_state=1847).reset_index(drop=True))\n",
    "\n",
    "for i, jsonset in enumerate(jsonsets):\n",
    "    jsonset['g_id'] = {\n",
    "      \"fluency\": [0,1,2,3,4,5],\n",
    "      \"adequacy\": [1,2]\n",
    "    }  \n",
    "    \n",
    "    jsonset['claim_id'] = jsonset['claim_id'] '_GD_PLUS_ADEQUACY'    \n",
    "    jsonset['verbalised_claim'] = jsonsets_2[i]['verbalised_claim']\n",
    "\n",
    "with open('data/campaign/batch_1/verbalisations_gd_plus_adequacy.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets[:golden_standard_quota], f, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are first converting campaign batch 1! That is campaign groups 0, 1, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_batch_1 = data_df[data_df.campaign_group < 3].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21 %                \r",
      "0.42 %                \r",
      "0.63 %                \r",
      "0.84 %                \r",
      "1.05 %                \r",
      "1.26 %                \r",
      "1.47 %                \r",
      "1.68 %                \r",
      "1.89 %                \r",
      "2.1 %                \r",
      "2.31 %                \r",
      "2.52 %                \r",
      "2.73 %                \r",
      "2.94 %                \r",
      "3.14 %                \r",
      "3.35 %                \r",
      "3.56 %                \r",
      "3.77 %                \r",
      "3.98 %                \r",
      "4.19 %                \r",
      "4.4 %                \r",
      "4.61 %                \r",
      "4.82 %                \r",
      "5.03 %                \r",
      "5.24 %                \r",
      "5.45 %                \r",
      "5.66 %                \r",
      "5.87 %                \r",
      "6.08 %                \r",
      "6.29 %                \r",
      "6.5 %                \r",
      "6.71 %                \r",
      "6.92 %                \r",
      "7.13 %                \r",
      "7.34 %                \r",
      "7.55 %                \r",
      "7.76 %                \r",
      "7.97 %                \r",
      "8.18 %                \r",
      "8.39 %                \r",
      "8.6 %                \r",
      "8.81 %                \r",
      "9.01 %                \r",
      "9.22 %                \r",
      "9.43 %                \r",
      "9.64 %                \r",
      "9.85 %                \r",
      "10.06 %                \r",
      "10.27 %                \r",
      "10.48 %                \r",
      "10.69 %                \r",
      "10.9 %                \r",
      "11.11 %                \r",
      "11.32 %                \r",
      "11.53 %                \r",
      "11.74 %                \r",
      "11.95 %                \r",
      "12.16 %                \r",
      "12.37 %                \r",
      "12.58 %                \r",
      "12.79 %                \r",
      "13.0 %                \r",
      "13.21 %                \r",
      "13.42 %                \r",
      "13.63 %                \r",
      "13.84 %                \r",
      "14.05 %                \r",
      "14.26 %                \r",
      "14.47 %                \r",
      "14.68 %                \r",
      "14.88 %                \r",
      "15.09 %                \r",
      "15.3 %                \r",
      "15.51 %                \r",
      "15.72 %                \r",
      "15.93 %                \r",
      "16.14 %                \r",
      "16.35 %                \r",
      "16.56 %                \r",
      "16.77 %                \r",
      "16.98 %                \r",
      "17.19 %                \r",
      "17.4 %                \r",
      "17.61 %                \r",
      "17.82 %                \r",
      "18.03 %                \r",
      "18.24 %                \r",
      "18.45 %                \r",
      "18.66 %                \r",
      "18.87 %                \r",
      "19.08 %                \r",
      "19.29 %                \r",
      "19.5 %                \r",
      "19.71 %                \r",
      "19.92 %                \r",
      "20.13 %                \r",
      "20.34 %                \r",
      "20.55 %                \r",
      "20.75 %                \r",
      "20.96 %                \r",
      "21.17 %                \r",
      "21.38 %                \r",
      "21.59 %                \r",
      "21.8 %                \r",
      "22.01 %                \r",
      "22.22 %                \r",
      "22.43 %                \r",
      "22.64 %                \r",
      "22.85 %                \r",
      "23.06 %                \r",
      "23.27 %                \r",
      "23.48 %                \r",
      "23.69 %                \r",
      "23.9 %                \r",
      "24.11 %                \r",
      "24.32 %                \r",
      "24.53 %                \r",
      "24.74 %                \r",
      "24.95 %                \r",
      "25.16 %                \r",
      "25.37 %                \r",
      "25.58 %                \r",
      "25.79 %                \r",
      "26.0 %                \r",
      "26.21 %                \r",
      "26.42 %                \r",
      "26.62 %                \r",
      "26.83 %                \r",
      "27.04 %                \r",
      "27.25 %                \r",
      "27.46 %                \r",
      "27.67 %                \r",
      "27.88 %                \r",
      "28.09 %                \r",
      "28.3 %                \r",
      "28.51 %                \r",
      "28.72 %                \r",
      "28.93 %                \r",
      "29.14 %                \r",
      "29.35 %                \r",
      "29.56 %                \r",
      "29.77 %                \r",
      "29.98 %                \r",
      "30.19 %                \r",
      "30.4 %                \r",
      "30.61 %                \r",
      "30.82 %                \r",
      "31.03 %                \r",
      "31.24 %                \r",
      "31.45 %                \r",
      "31.66 %                \r",
      "31.87 %                \r",
      "32.08 %                \r",
      "32.29 %                \r",
      "32.49 %                \r",
      "32.7 %                \r",
      "32.91 %                \r",
      "33.12 %                \r",
      "33.33 %                \r",
      "33.54 %                \r",
      "33.75 %                \r",
      "33.96 %                \r",
      "34.17 %                \r",
      "34.38 %                \r",
      "34.59 %                \r",
      "34.8 %                \r",
      "35.01 %                \r",
      "35.22 %                \r",
      "35.43 %                \r",
      "35.64 %                \r",
      "35.85 %                \r",
      "36.06 %                \r",
      "36.27 %                \r",
      "36.48 %                \r",
      "36.69 %                \r",
      "36.9 %                \r",
      "37.11 %                \r",
      "37.32 %                \r",
      "37.53 %                \r",
      "37.74 %                \r",
      "37.95 %                \r",
      "38.16 %                \r",
      "38.36 %                \r",
      "38.57 %                \r",
      "38.78 %                \r",
      "38.99 %                \r",
      "39.2 %                \r",
      "39.41 %                \r",
      "39.62 %                \r",
      "39.83 %                \r",
      "40.04 %                \r",
      "40.25 %                \r",
      "40.46 %                \r",
      "40.67 %                \r",
      "40.88 %                \r",
      "41.09 %                \r",
      "41.3 %                \r",
      "41.51 %                \r",
      "41.72 %                \r",
      "41.93 %                \r",
      "42.14 %                \r",
      "42.35 %                \r",
      "42.56 %                \r",
      "42.77 %                \r",
      "42.98 %                \r",
      "43.19 %                \r",
      "43.4 %                \r",
      "43.61 %                \r",
      "43.82 %                \r",
      "44.03 %                \r",
      "44.23 %                \r",
      "44.44 %                \r",
      "44.65 %                \r",
      "44.86 %                \r",
      "45.07 %                \r",
      "45.28 %                \r",
      "45.49 %                \r",
      "45.7 %                \r",
      "45.91 %                \r",
      "46.12 %                \r",
      "46.33 %                \r",
      "46.54 %                \r",
      "46.75 %                \r",
      "46.96 %                \r",
      "47.17 %                \r",
      "47.38 %                \r",
      "47.59 %                \r",
      "47.8 %                \r",
      "48.01 %                \r",
      "48.22 %                \r",
      "48.43 %                \r",
      "48.64 %                \r",
      "48.85 %                \r",
      "49.06 %                \r",
      "49.27 %                \r",
      "49.48 %                \r",
      "49.69 %                \r",
      "49.9 %                \r",
      "50.1 %                \r",
      "50.31 %                \r",
      "50.52 %                \r",
      "50.73 %                \r",
      "50.94 %                \r",
      "51.15 %                \r",
      "51.36 %                \r",
      "51.57 %                \r",
      "51.78 %                \r",
      "51.99 %                \r",
      "52.2 %                \r",
      "52.41 %                \r",
      "52.62 %                \r",
      "52.83 %                \r",
      "53.04 %                \r",
      "53.25 %                \r",
      "53.46 %                \r",
      "53.67 %                \r",
      "53.88 %                \r",
      "54.09 %                \r",
      "54.3 %                \r",
      "54.51 %                \r",
      "54.72 %                \r",
      "54.93 %                \r",
      "55.14 %                \r",
      "55.35 %                \r",
      "55.56 %                \r",
      "55.77 %                \r",
      "55.97 %                \r",
      "56.18 %                \r",
      "56.39 %                \r",
      "56.6 %                \r",
      "56.81 %                \r",
      "57.02 %                \r",
      "57.23 %                \r",
      "57.44 %                \r",
      "57.65 %                \r",
      "57.86 %                \r",
      "58.07 %                \r",
      "58.28 %                \r",
      "58.49 %                \r",
      "58.7 %                \r",
      "58.91 %                \r",
      "59.12 %                \r",
      "59.33 %                \r",
      "59.54 %                \r",
      "59.75 %                \r",
      "59.96 %                \r",
      "60.17 %                \r",
      "60.38 %                \r",
      "60.59 %                \r",
      "60.8 %                \r",
      "61.01 %                \r",
      "61.22 %                \r",
      "61.43 %                \r",
      "61.64 %                \r",
      "61.84 %                \r",
      "62.05 %                \r",
      "62.26 %                \r",
      "62.47 %                \r",
      "62.68 %                \r",
      "62.89 %                \r",
      "63.1 %                \r",
      "63.31 %                \r",
      "63.52 %                \r",
      "63.73 %                \r",
      "63.94 %                \r",
      "64.15 %                \r",
      "64.36 %                \r",
      "64.57 %                \r",
      "64.78 %                \r",
      "64.99 %                \r",
      "65.2 %                \r",
      "65.41 %                \r",
      "65.62 %                \r",
      "65.83 %                \r",
      "66.04 %                \r",
      "66.25 %                \r",
      "66.46 %                \r",
      "66.67 %                \r",
      "66.88 %                \r",
      "67.09 %                \r",
      "67.3 %                \r",
      "67.51 %                \r",
      "67.71 %                \r",
      "67.92 %                \r",
      "68.13 %                \r",
      "68.34 %                \r",
      "68.55 %                \r",
      "68.76 %                \r",
      "68.97 %                \r",
      "69.18 %                \r",
      "69.39 %                \r",
      "69.6 %                \r",
      "69.81 %                \r",
      "70.02 %                \r",
      "70.23 %                \r",
      "70.44 %                \r",
      "70.65 %                \r",
      "70.86 %                \r",
      "71.07 %                \r",
      "71.28 %                \r",
      "71.49 %                \r",
      "71.7 %                \r",
      "71.91 %                \r",
      "72.12 %                \r",
      "72.33 %                \r",
      "72.54 %                \r",
      "72.75 %                \r",
      "72.96 %                \r",
      "73.17 %                \r",
      "73.38 %                \r",
      "73.58 %                \r",
      "73.79 %                \r",
      "74.0 %                \r",
      "74.21 %                \r",
      "74.42 %                \r",
      "74.63 %                \r",
      "74.84 %                \r",
      "75.05 %                \r",
      "75.26 %                \r",
      "75.47 %                \r",
      "75.68 %                \r",
      "75.89 %                \r",
      "76.1 %                \r",
      "76.31 %                \r",
      "76.52 %                \r",
      "76.73 %                \r",
      "76.94 %                \r",
      "77.15 %                \r",
      "77.36 %                \r",
      "77.57 %                \r",
      "77.78 %                \r",
      "77.99 %                \r",
      "78.2 %                \r",
      "78.41 %                \r",
      "78.62 %                \r",
      "78.83 %                \r",
      "79.04 %                \r",
      "79.25 %                \r",
      "79.45 %                \r",
      "79.66 %                \r",
      "79.87 %                \r",
      "80.08 %                \r",
      "80.29 %                \r",
      "80.5 %                \r",
      "80.71 %                \r",
      "80.92 %                \r",
      "81.13 %                \r",
      "81.34 %                \r",
      "81.55 %                \r",
      "81.76 %                \r",
      "81.97 %                \r",
      "82.18 %                \r",
      "82.39 %                \r",
      "82.6 %                \r",
      "82.81 %                \r",
      "83.02 %                \r",
      "83.23 %                \r",
      "83.44 %                \r",
      "83.65 %                \r",
      "83.86 %                \r",
      "84.07 %                \r",
      "84.28 %                \r",
      "84.49 %                \r",
      "84.7 %                \r",
      "84.91 %                \r",
      "85.12 %                \r",
      "85.32 %                \r",
      "85.53 %                \r",
      "85.74 %                \r",
      "85.95 %                \r",
      "86.16 %                \r",
      "86.37 %                \r",
      "86.58 %                \r",
      "86.79 %                \r",
      "87.0 %                \r",
      "87.21 %                \r",
      "87.42 %                \r",
      "87.63 %                \r",
      "87.84 %                \r",
      "88.05 %                \r",
      "88.26 %                \r",
      "88.47 %                \r",
      "88.68 %                \r",
      "88.89 %                \r",
      "89.1 %                \r",
      "89.31 %                \r",
      "89.52 %                \r",
      "89.73 %                \r",
      "89.94 %                \r",
      "90.15 %                \r",
      "90.36 %                \r",
      "90.57 %                \r",
      "90.78"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " %                \r",
      "90.99 %                \r",
      "91.19 %                \r",
      "91.4 %                \r",
      "91.61 %                \r",
      "91.82 %                \r",
      "92.03 %                \r",
      "92.24 %                \r",
      "92.45 %                \r",
      "92.66 %                \r",
      "92.87 %                \r",
      "93.08 %                \r",
      "93.29 %                \r",
      "93.5 %                \r",
      "93.71 %                \r",
      "93.92 %                \r",
      "94.13 %                \r",
      "94.34 %                \r",
      "94.55 %                \r",
      "94.76 %                \r",
      "94.97 %                \r",
      "95.18 %                \r",
      "95.39 %                \r",
      "95.6 %                \r",
      "95.81 %                \r",
      "96.02 %                \r",
      "96.23 %                \r",
      "96.44 %                \r",
      "96.65 %                \r",
      "96.86 %                \r",
      "97.06 %                \r",
      "97.27 %                \r",
      "97.48 %                \r",
      "97.69 %                \r",
      "97.9 %                \r",
      "98.11 %                \r",
      "98.32 %                \r",
      "98.53 %                \r",
      "98.74 %                \r",
      "98.95 %                \r",
      "99.16 %                \r",
      "99.37 %                \r",
      "99.58 %                \r",
      "99.79 %                \r",
      "100.0 %                \r"
     ]
    }
   ],
   "source": [
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_standard_quota = 45\n",
    "\n",
    "random.shuffle(jsonsets)\n",
    "with open('data/campaign/batch_1/verbalisations_gd.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets[:golden_standard_quota], f, indent=2,ensure_ascii=False)\n",
    "with open('data/campaign/batch_1/verbalisations.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets[golden_standard_quota:], f, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets\n",
      "Gd len: 45\n",
      "Gd plus len: 45\n",
      "Non-Gd len: 432\n",
      "Generated 108 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 4, 3, 4, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 4, 2, 4, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2]\n",
      "Generating Task Sets\n",
      "Gd len: 45\n",
      "Gd plus len: 45\n",
      "Non-Gd len: 432\n",
      "Generated 108 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 4, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 5, 2, 2, 2, 2, 3, 4, 2, 2, 3, 2, 4, 2, 2, 2, 2, 3, 4, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 4, 2, 2, 3, 3, 2, 2, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "for task_type in ['fluency','adequacy']:\n",
    "\n",
    "    print(f'Generating Task Sets for {task_type}')\n",
    "    with open('data/campaign/batch_1/verbalisations_gd.json','r',encoding='utf8') as f:\n",
    "        verbalisations_gd = json.load(f)\n",
    "        print('Gd len:',len(verbalisations_gd))\n",
    "    with open(f'data/campaign/batch_1/verbalisations_gd_plus_{task_type}.json','r',encoding='utf8') as f:\n",
    "        verbalisations_gd_plus = json.load(f)\n",
    "        print('Gd plus len:',len(verbalisations_gd))\n",
    "    with open('data/campaign/batch_1/verbalisations.json','r',encoding='utf8') as f:\n",
    "        verbalisations = json.load(f)\n",
    "        print('Non-Gd len:',len(verbalisations))\n",
    "\n",
    "    maxSingleInstances = 1 #MAX TIMES ANY REFERENCE APPEARS AMONG THE TASK SETS\n",
    "    maxSingleInstances_gd = 2\n",
    "\n",
    "    ds, c, c_gd = generateIDdTaskSets(verbalisations, verbalisations_gd + verbalisations_gd_plus, maxSingleInstances, maxSingleInstances_gd)\n",
    "    print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "    print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "\n",
    "\n",
    "    with open(f'./data/campaign/batch_1/TaskSets_{task_type}.json','w+',encoding='utf8') as f:\n",
    "        json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are now converting campaign batch 2! That is campaign groups 10,15,20,25,30,35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cgs = [10,15,20,25,30,35]\n",
    "data_df_batch_2 = data_df[data_df.campaign_group.apply(lambda x : x in selected_cgs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 %                 %                \r"
     ]
    }
   ],
   "source": [
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_batch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE WILL USE THE SAME GOLD DATA AS FOR BATCH 1, NO NEED TO ANNOTATE MORE IF THOSE WORKED\n",
    "\n",
    "random.shuffle(jsonsets)\n",
    "with open('data/campaign/batch_2/verbalisations.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets, f, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets for fluency\n",
      "Gd len: 45\n",
      "Gd plus len: 45\n",
      "Non-Gd len: 949\n",
      "Generated 238 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [5, 5, 5, 5, 5, 5, 6, 6, 5, 6, 5, 5, 5, 5, 5, 5, 5, 8, 5, 5, 5, 5, 6, 5, 5, 6, 6, 6, 6, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 6, 5, 5, 5, 6, 5, 6, 6, 5, 5, 5, 5, 5, 5, 6, 5, 6, 5, 6, 5, 6, 5]\n",
      "Generating Task Sets for adequacy\n",
      "Gd len: 45\n",
      "Gd plus len: 45\n",
      "Non-Gd len: 949\n",
      "Generated 238 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [6, 5, 5, 6, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 6, 5, 6, 6, 5, 5, 5, 5, 6, 5, 5, 6, 6, 5, 5, 5, 5, 5, 5, 6, 6, 6, 5, 5, 5, 5, 6, 5, 5, 6, 5, 6, 6, 5, 5, 5, 5, 6, 6, 5, 5, 6, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5, 5, 7, 6, 5, 5, 5, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "for task_type in ['fluency','adequacy']:\n",
    "\n",
    "    print(f'Generating Task Sets for {task_type}')\n",
    "    with open('data/campaign/batch_1/verbalisations_gd.json','r',encoding='utf8') as f:\n",
    "        verbalisations_gd = json.load(f)\n",
    "        print('Gd len:',len(verbalisations_gd))\n",
    "    with open(f'data/campaign/batch_1/verbalisations_gd_plus_{task_type}.json','r',encoding='utf8') as f:\n",
    "        verbalisations_gd_plus = json.load(f)\n",
    "        print('Gd plus len:',len(verbalisations_gd))\n",
    "    with open('data/campaign/batch_2/verbalisations.json','r',encoding='utf8') as f:\n",
    "        verbalisations = json.load(f)\n",
    "        print('Non-Gd len:',len(verbalisations))\n",
    "\n",
    "    maxSingleInstances = 1 #MAX TIMES ANY REFERENCE APPEARS AMONG THE TASK SETS\n",
    "    maxSingleInstances_gd = 5\n",
    "\n",
    "    ds, c, c_gd = generateIDdTaskSets(verbalisations, verbalisations_gd + verbalisations_gd_plus, maxSingleInstances, maxSingleInstances_gd)\n",
    "    print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "    print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "\n",
    "\n",
    "    with open(f'./data/campaign/batch_2/TaskSets_{task_type}.json','w+',encoding='utf8') as f:\n",
    "        json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954, -1930.17)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= 159*6\n",
    "x=\n",
    "\n",
    "balance = 220.83\n",
    "x, balance - np.ceil(x/4)*(0.5+1)*5*1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
